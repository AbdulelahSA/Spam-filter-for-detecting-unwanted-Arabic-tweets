{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C69YJnddSd_"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers openpyxl scikit-learn torch tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
        "from sklearn.metrics import  accuracy_score, precision_score, recall_score, f1_score\n",
        "import time"
      ],
      "metadata": {
        "id": "LZhY_8dDdTld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset\n",
        "file_path = \"Dataset of Arabic Spam and Ham Tweets.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Extract textual features using AraBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabert\")\n",
        "model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabert\")\n",
        "\n",
        "def extract_textual_features(texts):\n",
        "    tokens = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512, is_split_into_words=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "    features = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "    return features\n",
        "\n",
        "df['AraBERT Features'] = df['Cleaned Text'].apply(lambda x: extract_textual_features([str(x)]))\n",
        "\n"
      ],
      "metadata": {
        "id": "nHbt4DPqdTzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract supplementary statistical features\n",
        "statistical_features = df[['Retweet Count', 'Favorite Count']].values\n",
        "\n",
        "# Combine AraBERT features with statistical features\n",
        "combined_features = np.concatenate([df['AraBERT Features'].values.tolist(), statistical_features], axis=1)\n",
        "\n",
        "# Handle missing values by filling NaN with the mean of each column\n",
        "combined_features = np.nan_to_num(combined_features, nan=np.nanmean(combined_features, axis=0))\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(combined_features)\n",
        "\n",
        "#Encode the target variable\n",
        "le = LabelEncoder()\n",
        "df['Label'] = le.fit_transform(df['Label'])\n",
        "\n",
        "# Set up cross-validation\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n"
      ],
      "metadata": {
        "id": "GDy6WgD1A3ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"SVM\": SVC(random_state=42),\n",
        "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
        "    \"K Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"Neural Network\" : MLPClassifier(random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "# Define metrics\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score,\n",
        "    \"Precision\": precision_score,\n",
        "    \"Recall\": recall_score,\n",
        "    \"F1 Score\": f1_score\n",
        "}\n",
        "\n",
        "# Initialize an empty dictionary to store results\n",
        "results = {}\n",
        "time_results = {}\n",
        "\n",
        "# Perform cross-validation and calculate metrics for each classifier\n",
        "for clf_name, clf in classifiers.items():\n",
        "    clf_results = []\n",
        "    clf_time_results = []\n",
        "    time_values = {\n",
        "        \"Train Time (s)\": [],\n",
        "        \"Prediction Time (s)\": []\n",
        "    }\n",
        "    metric_values = {\n",
        "        \"Accuracy\": [],\n",
        "        \"Precision\": [],\n",
        "        \"Recall\": [],\n",
        "        \"F1 Score\": []\n",
        "    }\n",
        "    # Perform cross-validation predictions\n",
        "    for fold, (train_idx, test_idx) in enumerate(cv.split(X_scaled)):\n",
        "\n",
        "        X_train_fold, X_test_fold = X_scaled[train_idx], X_scaled[test_idx]\n",
        "        y_train_fold, y_test_fold = df['Label'].iloc[train_idx], df['Label'].iloc[test_idx]\n",
        "\n",
        "        start_time = time.time()\n",
        "        clf.fit(X_train_fold, y_train_fold)\n",
        "        mid_time = time.time()\n",
        "        y_pred_fold = clf.predict(X_test_fold)\n",
        "        end_time = time.time()\n",
        "\n",
        "        time_values[\"Train Time (s)\"].append(mid_time - start_time)\n",
        "        time_values[\"Prediction Time (s)\"].append(end_time - mid_time)\n",
        "\n",
        "        for metric_name, metric_func in metrics.items():\n",
        "            metric_value = metric_func(y_test_fold, y_pred_fold)\n",
        "            metric_values[metric_name].append(metric_value)\n",
        "\n",
        "    # Calculate average metric value across folds\n",
        "    for metric_name, metric_value in metric_values.items():\n",
        "        average_metric_value = np.mean(metric_value)\n",
        "        clf_results.append(average_metric_value)\n",
        "\n",
        "    for time_name, time_value in time_values.items():\n",
        "        average_time_value = np.mean(time_value)\n",
        "        clf_time_results.append(average_time_value)\n",
        "\n",
        "    results[clf_name] = clf_results\n",
        "    time_results[clf_name] = clf_time_results + [clf_time_results[0]+clf_time_results[1]]\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "metrics_keys = list(metrics.keys())\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index', columns=metrics_keys)\n",
        "results_df.index.name = 'Classifier'\n",
        "\n",
        "time_names = [\"Train Time (s)\",\"Prediction Time (s)\",\"Totoal Time (s)\"]\n",
        "time_results_df = pd.DataFrame.from_dict(time_results, orient='index', columns=time_names)\n",
        "time_results_df.index.name = 'Classifier'\n",
        "\n",
        "print(results_df)\n",
        "print()\n",
        "print(time_results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEX30WaMdT_E",
        "outputId": "dd8a8742-624f-4968-9aa6-8297130614ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      Accuracy  Precision    Recall  F1 Score\n",
            "Classifier                                                   \n",
            "Random Forest         0.943673   0.966667  0.854960  0.905275\n",
            "SVM                   0.951837   0.948095  0.906944  0.925752\n",
            "Gaussian Naive Bayes  0.895429   0.838254  0.839087  0.834492\n",
            "Decision Tree         0.899592   0.823271  0.868651  0.844387\n",
            "Logistic Regression   0.963837   0.958095  0.929167  0.942151\n",
            "K Nearest Neighbors   0.939592   0.922374  0.887698  0.901567\n",
            "Neural Network        0.963918   0.928640  0.968452  0.947231\n",
            "AdaBoost              0.975918   0.964706  0.963690  0.961908\n",
            "Gradient Boosting     0.907673   0.863791  0.848016  0.853884\n",
            "XGBoost               0.959755   0.946184  0.928571  0.935643\n",
            "\n",
            "                      Train Time (s)  Prediction Time (s)  Totoal Time (s)\n",
            "Classifier                                                                \n",
            "Random Forest               0.339593             0.005676         0.345269\n",
            "SVM                         0.013493             0.005054         0.018547\n",
            "Gaussian Naive Bayes        0.003299             0.000797         0.004096\n",
            "Decision Tree               0.053174             0.000530         0.053703\n",
            "Logistic Regression         0.023746             0.000445         0.024191\n",
            "K Nearest Neighbors         0.001087             0.007275         0.008362\n",
            "Neural Network              0.293423             0.000980         0.294402\n",
            "AdaBoost                    1.148531             0.011155         1.159686\n",
            "Gradient Boosting           4.464749             0.001019         4.465768\n",
            "XGBoost                     0.583170             0.001738         0.584908\n"
          ]
        }
      ]
    }
  ]
}